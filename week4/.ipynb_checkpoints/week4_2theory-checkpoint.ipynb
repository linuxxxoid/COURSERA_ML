{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Методы понижения размерности.\n",
    "\n",
    "Первый метод — это метод главных компонент,\n",
    "\n",
    "вторая группа методов — это методы многомерного шкалирования\n",
    "\n",
    "третий метод — метод t-SNE.\n",
    "\n",
    "Понижение размерности - это уменьшение количества признаков\n",
    "\n",
    "Понижение размерности можно использовать для удобной визуализации\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод Главных Компонент (Principal)\n",
    "\n",
    "- новое признаковое пространство\n",
    "\n",
    "- новые оси координат (главные компоненты) - ортогональны\n",
    "\n",
    "- знаения новых признаков: линейная комбинация предыдущих признаков\n",
    "    \n",
    "    $z_1(i) = a_1 x_1(i) + a_2x_2(i) + \\dots + a_D x_D (i) = a^T x$\n",
    "    \n",
    "- новые признаки должны сохранить как можно больше дисперсии, как можно больше изменчивости и вариативности наших исходных данных. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть у нас есть некоторая квадратная матрица А. Рассмотрим такое соотношение. А умножить на вектор v равно лямбда v, где v это некоторый не нулевой вектор, а лямбда это число. Вектор v будет называться собственным вектором матрицы А, а, собственно, число А будет называться собственным числом матрицы А. Векторы и, собственно, числа не единственное для матрицы. Более того, для одного собственного числа может быть несколько собственных векторов.\n",
    "\n",
    "Ковариации - это мера линейной зависимости двух случайных величин\n",
    "\n",
    "Корреляция - это просто нормированная ковариация.\n",
    "\n",
    "$C$ -  матрица ковариаций\n",
    "\n",
    "$C_{jk} = \\frac{1}{n-1} \\sum_{i=1}^{N} (X_{ij} - \\hat X_{j})(X_{ik} - \\hat X_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "на диагонали этой матрица - дисперсии соотв-х признаков\n",
    "\n",
    "дополнительно предполагая, что все наши признаки центрированы, то есть их среднее значение равно нулю, матрицу ковариаций можно записать в более компактном виде.\n",
    "\n",
    "для любого $j: \\hat X_j = 0$\n",
    "\n",
    "$с = \\frac {1}{n-1} X^T X$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для матрицы ковариации S\n",
    "\n",
    "- собственные числа $ \\lambda_i \\geq 0$\n",
    "\n",
    "заранее договоримся, что собственные числа мы будем упорядочивать по убыванию. Кроме того собственные вектора при разных собственных числах будут ортогональны друг к другу. То есть их скалярное произведение будет равно нулю. \n",
    "\n",
    "собственные вектора при $\\lambda_j \\neq \\lambda_k$ ортогональны $v_j^T v_k = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Вывод компонент\n",
    "\n",
    "- Хотим получить линейное преобразование исходных признаков\n",
    "\n",
    "Z = XA = X "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
