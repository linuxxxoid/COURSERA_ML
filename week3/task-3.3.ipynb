{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ансамблевые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача классификации "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом практическом задании вы научитесь работать с ансамблевыми моделями. Мы начнем с задачи классификации итальянского вина на предмет его принадлежности к одному из трех видов. Загрузите датасет `Wine Data Database` с помощью функции `load_wine` из модуля `sklearn.datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "X, y = load_wine(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель случайного леса для классификации представлена классом `RandomForestClassifier` из модуля `sklearn.ensemble`. Конструктор этого класса содержит аргумент `n_estimators`, который соответствует колличеству базовых алгоритмов в случайном лесе. Целью этого задания будет настройка этого параметра. Сравните модели случайных лесов с различным числом базовых алгоритмов `{1, 5, 10, 20}`. Что происходит с качеством случайного леса на тестовых данных при увеличении этого числа? Ответом на это задание `answer1` является лучшая оценка качества модели, округленная до трех знаков после запятой. Используйте `accuracy` как метрику качества и скользящий контроль `cross_val_score` как метод оценки качества модели. Установите параметр `cv = StratifiedKFold(4)`. Возьмите среднее значение оценки качества. Для каждой из моделей случайного леса используете `random_state=42` при создании нового экземпляра."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *РЕШЕНИЕ*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "= number of base alg: 1 =\n",
      "Cross val score: [0.68888889 0.86666667 0.86363636 0.88636364]\n",
      "Mean cross val score: 0.826\n",
      "Answer1: 0.826\n",
      "\n",
      "= number of base alg: 5 =\n",
      "Cross val score: [0.88888889 0.93333333 0.95454545 0.97727273]\n",
      "Mean cross val score: 0.939\n",
      "Answer1: 0.939\n",
      "\n",
      "= number of base alg: 10 =\n",
      "Cross val score: [0.93333333 1.         0.97727273 0.97727273]\n",
      "Mean cross val score: 0.972\n",
      "Answer1: 0.972\n",
      "\n",
      "= number of base alg: 20 =\n",
      "Cross val score: [0.95555556 0.95555556 1.         0.97727273]\n",
      "Mean cross val score: 0.972\n",
      "Answer1: 0.972\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "\n",
    "base_alg = [1, 5, 10, 20]\n",
    "\n",
    "cv = StratifiedKFold(4)\n",
    "\n",
    "answer1 = 0\n",
    "\n",
    "for num_base_alg in base_alg:\n",
    "    rfc = RandomForestClassifier(n_estimators = num_base_alg, random_state = 42)\n",
    "    model = rfc.fit(X, y)\n",
    "\n",
    "    cv_score = cross_val_score(\n",
    "    rfc, X, y,\n",
    "    scoring = 'accuracy', cv = cv)\n",
    "    \n",
    "    if answer1 < cv_score.mean():\n",
    "        answer1 = cv_score.mean()\n",
    "    \n",
    "    print('\\n= number of base alg: {} ='.format(num_base_alg))\n",
    "    print('Cross val score: {}'.format(cv_score))\n",
    "    print('Mean cross val score: {:.3f}'.format(cv_score.mean()))\n",
    "    print('Answer1: {:.3f}'.format(answer1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее сравните модель градиентного бустинга `GradientBoostingClassifier` из `sklearn.ensemble` с логистической регрессией `LogisticRegression` из `sklearn.linear_model` на этой выборке. Используете параметр `random_state=42` при создании экземпляров классов. Какая из моделей работает лучше? Приведите лучшую оценку, округленную до трех знаков после запятой, в качестве ответа `answer2` на это задание. Какие выводы из этого можно сделать?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *РЕШЕНИЕ*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val score: [0.82222222 0.91111111 0.97727273 0.97727273]\n",
      "Mean cross val score: 0.922\n",
      "Answer2: 0.922\n",
      "\n",
      "\n",
      "Cross val score: [0.88888889 0.93333333 1.         1.        ]\n",
      "Mean cross val score: 0.956\n",
      "Answer2: 0.956\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linuxoid/.virtualenvs/dl4cv/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/linuxoid/.virtualenvs/dl4cv/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/linuxoid/.virtualenvs/dl4cv/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/linuxoid/.virtualenvs/dl4cv/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/linuxoid/.virtualenvs/dl4cv/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "answer2 = 0\n",
    "\n",
    "\n",
    "gbc = GradientBoostingClassifier(random_state = 42)\n",
    "lr = LogisticRegression(random_state = 42)\n",
    "\n",
    "models = [gbc, lr]\n",
    "\n",
    "for model in models:\n",
    "    res = model.fit(X, y)\n",
    "\n",
    "    cv_score = cross_val_score(\n",
    "    model, X, y,\n",
    "    scoring = 'accuracy', cv = cv)\n",
    "    \n",
    "    if answer2 < cv_score.mean():\n",
    "        answer2 = cv_score.mean()\n",
    "    \n",
    "    print('Cross val score: {}'.format(cv_score))\n",
    "    print('Mean cross val score: {:.3f}'.format(cv_score.mean()))\n",
    "    print('Answer2: {:.3f}\\n\\n'.format(answer2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузите уже известную вам выборку `Boston House Prices` и разделите ее случайным образом на тренировочную и тестовую выборку. Для этого используете функцию `train_test_split` с параметрами `random_state=54` и `test_size=0.33`. Мы будем сравнивать 4 модели: `RandomForestRegressor`, `GradientBoostingRegressor` из `sklearn.ensemble`, а так же Гребневую регрессию и ЛАССО (`Ridge`, `Lasso` из `sklearn.linear_model`). Обучите каждую модель на тренировочной выборке с параметром `random_state=42` в конструкторе. Какая из моделей показывает наименьшее значение среднеквадратической ошибки на тестовых данных? В качестве ответа `answer3` приведите это значение, округленное до двух цифр после запятой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *РЕШЕНИЕ*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
      "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                      max_samples=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
      "                      random_state=42, verbose=0, warm_start=False)\n",
      "\n",
      "\n",
      "MSE: 9.48\n",
      "\n",
      "min MSE: 9.48\n",
      "\n",
      "model: GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
      "                          init=None, learning_rate=0.1, loss='ls', max_depth=3,\n",
      "                          max_features=None, max_leaf_nodes=None,\n",
      "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                          min_samples_leaf=1, min_samples_split=2,\n",
      "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                          n_iter_no_change=None, presort='deprecated',\n",
      "                          random_state=42, subsample=1.0, tol=0.0001,\n",
      "                          validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "\n",
      "\n",
      "MSE: 8.54\n",
      "\n",
      "min MSE: 8.54\n",
      "\n",
      "model: Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "      normalize=False, random_state=42, solver='auto', tol=0.001)\n",
      "\n",
      "\n",
      "MSE: 23.80\n",
      "\n",
      "min MSE: 8.54\n",
      "\n",
      "model: Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=42,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n",
      "\n",
      "\n",
      "MSE: 26.92\n",
      "\n",
      "min MSE: 8.54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "answer3 = 100.0\n",
    "\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state = 54,\n",
    "    test_size = 0.33\n",
    ")\n",
    "\n",
    "rfc = RandomForestRegressor(random_state = 42)\n",
    "gbr = GradientBoostingRegressor(random_state = 42)\n",
    "\n",
    "ridge = Ridge(random_state = 42)\n",
    "lasso = Lasso(random_state = 42)\n",
    "\n",
    "for mod in [rfc, gbr, ridge, lasso]:\n",
    "    model = mod.fit(x_train, y_train)\n",
    "    pred = mod.predict(x_test)\n",
    "    \n",
    "    if answer3 > mean_squared_error(y_test, pred):\n",
    "        answer3 = mean_squared_error(y_test, pred)\n",
    "    \n",
    "    print('model: {}\\n'.format(mod))\n",
    "    print('\\nMSE: {:.2f}\\n'.format(mean_squared_error(y_test, pred)))\n",
    "    print('min MSE: {:.2f}\\n'.format(answer3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Строка с ответами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score (random forest) 0.972\n",
      "Best score (other algorithms) 0.956\n",
      "Best score (regression) 8.54\n"
     ]
    }
   ],
   "source": [
    "output = \"\"\"Best score (random forest) {0:.3f}\n",
    "Best score (other algorithms) {1:.3f}\n",
    "Best score (regression) {2:.2f}\"\"\"\n",
    "print(output.format(answer1, answer2, answer3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
