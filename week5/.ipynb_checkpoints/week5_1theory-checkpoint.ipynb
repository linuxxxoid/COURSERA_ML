{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сложности:\n",
    "    - сложность в интерпретации\n",
    "    - требуют большого количества данных\n",
    "    - требуют вычислительных ресурсов\n",
    "    - сложность в настройке\n",
    "    - переобучаются\n",
    "    - не всегда заводятся\n",
    "    - архитектуры для конткретных задач\n",
    "    - порог входа высок\n",
    "    \n",
    "причины недавней популярности нейронных сетей:\n",
    "+ Появление больших объемов данных, связанных с развитием интернет-технологий\n",
    "\n",
    "+ Развитие вычислительных мощностей и ресуров\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нейронная сеть - состоит из нейронов.\n",
    "\n",
    "$X_{out} = f(\\sum\\limits_{i} w_i x_i + b)$\n",
    "\n",
    "это вычислительная единица, которая принимает какое-то количество значений, какой-то вектор, и просто возвращает число. \n",
    "\n",
    "По сути, нейронная сеть состоит из большого количества таких нейронов, которые передают информацию друг другу. Таким образом нейронная сеть представляет собой просто супер позицию большого количества нелинейных функций. \n",
    "\n",
    "Итак, нейрон принимает на вход какой-то вектор, какие-то входные данные.\n",
    "\n",
    "Также у нейрона существуют веса, которые настраиваются при обучении, которые обновляются при обучении.\n",
    "\n",
    "Эти веса перемножаются с входными данными.\n",
    "\n",
    "Мы также можем добавить какое-то смещение, все это дело сложить и отправить в функцию активации. \n",
    "\n",
    "Таким образом мы поулчили какие-то входные данные, перемножили их с весами нейрона, и на выходе вернули число. \n",
    "\n",
    "Это число может быть большим и тогда считается, что нейрон активирован, зажжен, и он уверен в том, что какой-то объект присутствует во входных данных. \n",
    "\n",
    "Нейрон может бы не активирован, значение может быть маленьким и тогда считается, что какого-то объекта нет в наших данных. \n",
    "\n",
    "По сути нейронная сеть представляет собой просто большое количество нейронов, объединенных в слои.\n",
    "\n",
    "Существует входной слой, какой-то скрытый слой и выходной слой. \n",
    "\n",
    "Все это слои нейронов, и все эти нейроны принимают входные какие-то параметры, входные значения, перемножают их со своими весами и возвращают число дальше.\n",
    "\n",
    "Таким образом у нас нейронная сеть может моделировать иерархическую структуру, которая, собственно, и представляет собой наш мир. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "НС - это просто суперпозиция большого количества нелинейных функций, которая позволяет моделировать иерархическую структуру нашего с вами мира.\n",
    "\n",
    "Что это за нелинейные функции? Функции активации в нейроне. \n",
    "После того, как мы перемножили входные данные с нашими весами, все это дело сложили и добавили какое-то смещение, мы отправляем полученные векторы или тензор в функцию активации.\n",
    "\n",
    "Функция активации применяется в каждом нейроне. Они бывают разные. Одной из первых, одной из основных, является функция активации sigmoid, которая позволяет нейрону вернуть значение от 0 до 1.\n",
    "\n",
    "$sigmoid(x) = \\frac {1} {1 + e^{-x}}$\n",
    "\n",
    "Таким образом у нас все нейроны возвращают число от 0 до 1, если 1 то нейрон уверен в том, что присутствует объект какой-то во входных данных, и он зажжен. \n",
    "\n",
    "Если 0, соответственно, то нейрон не зажжен.\n",
    "\n",
    "Еще одной важной функцией активации является гиперболический тангенс, который позволяет распределять значения от -1 до 1. \n",
    "\n",
    "$tanh(x) = \\frac {e^{x} - e^{-x}} {e^{x} + e^{-x}}$\n",
    "\n",
    "Собственно, если у нас 1, то нейрон у нас зажжен.\n",
    "\n",
    "Функция активации, которая получает все больше и больше распространения и используется в большом количестве моделей сейчас, является нелинейная функция rectified linear unit (ReLU), которая ведет себя как линейная функция, если у нас значение на входе больше 0, либо просто зануляет данные, если у нас значение отрицательное. \n",
    "\n",
    "\n",
    "$f(x) = max(0, x)$\n",
    "\n",
    "\n",
    "Нейронные сети состоят из нейронов, которые получают на вход какие-то значения, какие-то параметры, перемножают их со своими весами, и отправляют все это дело в функцию активации. Результат работы фукнции активации это выход нейрона, просто какое-то число. Эти числа отправляются дальше по слоям, а нейронные сети состоят из большого количества слоев."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## обучение нейросети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "у наших нейронов, которые составляют нашу нейронную сеть, есть разные веса. Эти веса настраиваются при обучении таким образом, чтобы у нас нейронная сеть выдавала нужный нам результат. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "функция, функция потерь, мы можем посчитать ее градиент, но как нам обновить наши веса у наших нейронов? На помощь нам приходит основной и главный метод обучения нейронной сети на данный момент — это метод обратного распространения ошибки, который с помощью chain rule производной нашей функции потерь пробрасывает обратно в предыдущие слои значение ошибки при обучении."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "важно знать, что у нас, как и в любых других моделях машинного обучения, существует функция потерь, существуют методы градиентной оптимизации, которые позволяют прокидывать нашу ошибку обратно к нейронам и настраивать веса нейронов так, чтобы мы получали нужный результат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " существуют разные модификации градиентного спуска (градиентные оптимизации), например, Momentum или Adam, которые лучше использовать, которые быстрее и лучше сходятся. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### как улучшить обучение?\n",
    "\n",
    "- добавить больше данных (в тренировочную выборку)\n",
    "\n",
    "- augmentation (добавление) - модификация данных. Например, добавление тех же картинок, но повернутых на 180 градусов, с шумами и тд\n",
    "\n",
    "- сгенерировать новые элементы\n",
    "\n",
    "- Регуляризация\n",
    "        - L1/L2\n",
    "        \n",
    "        - Dropout\n",
    "        \n",
    "        - Batch normalization\n",
    "        \n",
    "        - Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Регуляризация позволяет добавлять дополнительные ограничения, дополнительные контрейнты на веса нейронов таким образом, чтобы они не были слишком большими, например.\n",
    "\n",
    "самых известных подходов к регуляризации является Dropout.\n",
    "\n",
    "нейронная сеть состоит из нейронов, из разных слоев.\n",
    "Dropout говорит о том, что мы при обучении можем выкидывать какие-то нейроны, например, на каждом слое мы можем выкидывать 50 % случайных нейронов.И таким образом, наша нейронная сеть будет обучаться лучше.\n",
    "\n",
    "Достаточно странно: у нас нейронная сеть, например, состоит из тысячи нейронов, мы каждый раз выкидываем просто 500 из них, то есть как бы мы уменьшаем мощность нашей сети. Тем не менее, все равно все работает. Почему же так происходит? На самом деле тут есть несколько интуиций.\n",
    "\n",
    "при обучении таким образом, когда у нас каждый нейрон может быть в какой-то момент выключен, нейроны учатся не обуславливаться друг на друга. Они пытаются действительно выучивать реальные данные, а не зависеть от своих предшественников.\n",
    "\n",
    "А еще одна интуиция, которая может быть даже понятнее, это то, что, когда мы обучаем нейронную сеть, где каждый может быть выкинут в какой-то момент, мы обучаем ансамблевую модель. Как вы знаете, ансамблевые модели работают лучше. Если мы обучаем нейронную сети с Dropout, то по сути мы обучаем большое количество нейронных сетей с разными нейронами. при инференсе, при предсказании, при использовании нашей нейронной сети мы как бы усредняем ансамбль большого количества нейронных сетей. Таким образом, наша нейронная сеть лучше обучается, лучше работает.\n",
    "\n",
    "Еще одним механизмом, которые стал популярен совсем недавно и применяется сейчас практически везде, является Batch normalization. Как вы знаете, входные данные нашей сети, нашей модели нужно нормализовать, чтобы данные были примерно одного порядка и чтобы с ними модели было легче работать.\n",
    "\n",
    "оказывается, данные нормализовать можно не только перед обучением, но и во время, например, между батчами. Об этом, собственно, и Batch normalization.\n",
    "\n",
    "Ну и самый простой метод регуляризации так называемой — это Early stopping, или ранняя остановка можно его назвать. Он говорит о том, что в принципе, когда вы обучаете вашу модель, в какой-то момент можно остановиться, можно дальше не продолжать. Почему можно дальше не продолжать? Например, потому что падает качество на тестовой выборке. Вы обучаетесь, у вас качество на train улучшается, а на тесте внезапно начало падать, то есть у вас ваша модель переобучается. В этот момент вы можете остановиться или откатиться чуть назад и сказать, что вот, собственно, эти параметры, вот эти настроенные веса мы будем дальше использовать, нам можно дальше не обучаться.\n",
    "\n",
    "Итак, чтобы обучить нейронную сеть, практически всегда и везде используется алгоритм обратного распространения ошибки (прокидывается градиент от функции по всем нейронам, по всем слоям.) - метод вычисления градиента, который используется при обновления весов нейронной сети в процессе обучения"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
